{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mounted-montana",
   "metadata": {},
   "source": [
    "## Implementing Chris Doenlen's 'Bot Or Not' Python Module\n",
    "\n",
    "Everything from `twitter_funcs.py` was cloned from Chris' [repository](https://github.com/scrapfishies/twitter-bot-detection).\n",
    "\n",
    "I will use this to label each user as 'bot' (boolean 1/0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cellular-techno",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'twitter_funcs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7a08f387c8be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtwitter_funcs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'twitter_funcs'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from twitter_funcs import *\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import tweepy\n",
    "\n",
    "from datetime import datetime\n",
    "from secrets import api_secret_key, api_key, bearer_token\n",
    "import re\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"pickle/n2_tokenized_eff.pick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-vessel",
   "metadata": {},
   "source": [
    "I'm getting a rate limit error. according to twitter site, I can lookup 300 users per 15 minutes. Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(user_ids, n):\n",
    "    \"\"\"Yield successive n-sized chunks from user_ids (iterable).\"\"\"\n",
    "    lst = list(user_ids)\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-village",
   "metadata": {},
   "source": [
    "Here's where we'll implement Chris Doenlen's 'Bot or Not' model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bot_model.pick\", \"rb\") as read_file:\n",
    "    xgb_model = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time.sleep(60*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifieds = {}\n",
    "# bot_probas = {}\n",
    "\n",
    "# all_users = list(data.user_id.unique())\n",
    "# print(f\"Total number of users to scrape: {len(all_users)}\")\n",
    "# exist = pd.read_csv(\"../data/user_stats.csv\")\n",
    "# exist_users = list(exist.user_id.unique())\n",
    "# print(f\"Number of users already scraped: {len(exist_users)}\")\n",
    "\n",
    "# user_ids = []\n",
    "# for user in exist_users:\n",
    "#     if user in all_users:\n",
    "#         continue\n",
    "#     else:\n",
    "#         user_ids.append(user)\n",
    "    \n",
    "# print(f\"Preparing to identify bots for {len(user_ids)} users...\")\n",
    "\n",
    "# user_id_chunks = list(chunks(user_ids, n=300))\n",
    "\n",
    "# # now get stats for new users\n",
    "\n",
    "# csv_file = open(\"../data/user_stats.csv\", \"a\")\n",
    "# csv_writer = csv.writer(csv_file)\n",
    "# csv_writer.writerow([\"user_id\", \"bot_proba\", \"verified\"])\n",
    "# for chunk in user_id_chunks:\n",
    "#     print(f\"Preparing chunk. Num users: {len(chunk)}\")\n",
    "#     for user_id in chunk:\n",
    "#         print(f\"Preparing user '{user_id}'\")\n",
    "        \n",
    "        \n",
    "#         auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "#         api = tweepy.API(auth)\n",
    "        \n",
    "#         try: # Gather features for bot/not bot model\n",
    "#             # Get user information from screen name\n",
    "#             user = api.get_user(user_id)\n",
    "\n",
    "#             # account features to return for predicton\n",
    "#             account_age_days = (datetime.now() - user.created_at).days\n",
    "#             verified = user.verified # will also use this in our data\n",
    "#             geo_enabled = user.geo_enabled\n",
    "#             default_profile = user.default_profile\n",
    "#             default_profile_image = user.default_profile_image\n",
    "#             favourites_count = user.favourites_count\n",
    "#             followers_count = user.followers_count\n",
    "#             friends_count = user.friends_count\n",
    "#             statuses_count = user.statuses_count\n",
    "#             average_tweets_per_day = np.round(statuses_count / account_age_days, 3)\n",
    "\n",
    "#             # manufactured features\n",
    "#             hour_created = int(user.created_at.strftime(\"%H\"))\n",
    "#             network = np.round(np.log(1 + friends_count) * np.log(1 + followers_count), 3)\n",
    "#             tweet_to_followers = np.round(\n",
    "#                 np.log(1 + statuses_count) * np.log(1 + followers_count), 3\n",
    "#             )\n",
    "#             follower_acq_rate = np.round(\n",
    "#                 np.log(1 + (followers_count / account_age_days)), 3\n",
    "#             )\n",
    "#             friends_acq_rate = np.round(np.log(1 + (friends_count / account_age_days)), 3)\n",
    "\n",
    "#             # organizing list to be returned\n",
    "#             account_features = [\n",
    "#                 verified, hour_created,geo_enabled,default_profile,default_profile_image,favourites_count,\n",
    "#                 followers_count,friends_count,statuses_count,average_tweets_per_day,network,tweet_to_followers,\n",
    "#                 follower_acq_rate,friends_acq_rate]\n",
    "\n",
    "#             if account_features == np.nan:\n",
    "#                 proba = np.nan\n",
    "#                 verified = np.nan\n",
    "#                 csv_writer.writerow([user_id, proba, verified])\n",
    "#                 continue\n",
    "\n",
    "#             else:\n",
    "#                 user_m = np.matrix(account_features)\n",
    "#                 proba = np.round(xgb_model.predict_proba(user_m)[:, 1][0] * 100, 2)\n",
    "#                 verified = account_features[0]\n",
    "#                 csv_writer.writerow([user_id, proba, verified])\n",
    "\n",
    "#         except:\n",
    "#             print(f'error encountered, skipping user {user_id}')\n",
    "#             proba = np.nan\n",
    "#             verified = np.nan\n",
    "        \n",
    "#             csv_writer.writerow([user_id, proba, verified])\n",
    "#     print(\"Chunk complete. Waiting 15 minutes.\")\n",
    "#     time.sleep(15*60+1)\n",
    "\n",
    "# csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-breeding",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-off copy\n",
    "time.sleep(120)\n",
    "verifieds = {}\n",
    "bot_probas = {}\n",
    "\n",
    "user_ids = list(pd.read_pickle(\"pickle/grab_users.pick\"))\n",
    "    \n",
    "\n",
    "user_id_chunks = list(chunks(user_ids, n=300))\n",
    "\n",
    "# now get stats for new users\n",
    "\n",
    "csv_file = open(\"../data/user_stats.csv\", \"a\")\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow([\"user_id\", \"bot_proba\", \"verified\"])\n",
    "for chunk in user_id_chunks:\n",
    "    print(f\"Preparing chunk. Num users: {len(chunk)}\")\n",
    "    for user_id in chunk:\n",
    "        print(f\"Preparing user '{user_id}'\")\n",
    "        \n",
    "        \n",
    "        auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "        api = tweepy.API(auth)\n",
    "        \n",
    "        try: # Gather features for bot/not bot model\n",
    "            # Get user information from screen name\n",
    "            user = api.get_user(user_id)\n",
    "\n",
    "            # account features to return for predicton\n",
    "            account_age_days = (datetime.now() - user.created_at).days\n",
    "            verified = user.verified # will also use this in our data\n",
    "            geo_enabled = user.geo_enabled\n",
    "            default_profile = user.default_profile\n",
    "            default_profile_image = user.default_profile_image\n",
    "            favourites_count = user.favourites_count\n",
    "            followers_count = user.followers_count\n",
    "            friends_count = user.friends_count\n",
    "            statuses_count = user.statuses_count\n",
    "            average_tweets_per_day = np.round(statuses_count / account_age_days, 3)\n",
    "\n",
    "            # manufactured features\n",
    "            hour_created = int(user.created_at.strftime(\"%H\"))\n",
    "            network = np.round(np.log(1 + friends_count) * np.log(1 + followers_count), 3)\n",
    "            tweet_to_followers = np.round(\n",
    "                np.log(1 + statuses_count) * np.log(1 + followers_count), 3\n",
    "            )\n",
    "            follower_acq_rate = np.round(\n",
    "                np.log(1 + (followers_count / account_age_days)), 3\n",
    "            )\n",
    "            friends_acq_rate = np.round(np.log(1 + (friends_count / account_age_days)), 3)\n",
    "\n",
    "            # organizing list to be returned\n",
    "            account_features = [\n",
    "                verified, hour_created,geo_enabled,default_profile,default_profile_image,favourites_count,\n",
    "                followers_count,friends_count,statuses_count,average_tweets_per_day,network,tweet_to_followers,\n",
    "                follower_acq_rate,friends_acq_rate]\n",
    "\n",
    "            if account_features == np.nan:\n",
    "                proba = np.nan\n",
    "                verified = np.nan\n",
    "                csv_writer.writerow([user_id, proba, verified])\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                user_m = np.matrix(account_features)\n",
    "                proba = np.round(xgb_model.predict_proba(user_m)[:, 1][0] * 100, 2)\n",
    "                verified = account_features[0]\n",
    "                csv_writer.writerow([user_id, proba, verified])\n",
    "\n",
    "        except:\n",
    "            print(f'error encountered, skipping user {user_id}')\n",
    "            proba = np.nan\n",
    "            verified = np.nan\n",
    "        \n",
    "            csv_writer.writerow([user_id, proba, verified])\n",
    "    print(\"Chunk complete. Waiting 15 minutes.\")\n",
    "    time.sleep(15*60+1)\n",
    "\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "verifieds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['verified'] = data['user_id'].map(verifieds)\n",
    "data['bot_proba'] = data['user_id'].map(bot_probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-produce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
