{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "authorized-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "import spacy\n",
    "import contextualSpellCheck\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-holiday",
   "metadata": {},
   "source": [
    "**Kelsey 1-1**\n",
    "\n",
    "- cleaning\n",
    "    - preprocessing until comfortable with words\n",
    "   \n",
    "- sentiment analysis on all tweets\n",
    "    - don't need to do any splitting at this stage\n",
    "    - TextBlob & VaderSentiment first, spacy if the results aren't as expected\n",
    "    \n",
    "- topic modeling\n",
    "    - decide: use all tweets (all topics) at once\n",
    "        - start here\n",
    "        - then can use these as features in the dataFrame and do splitting here\n",
    "    - or: split to trump/biden - then bot/not bot for each\n",
    "    - point here is there are multiple ways to split it\n",
    "        - no right answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "integral-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 40)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "inner-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must uncomment & run the first time to DOWNLOAD NLTK data\n",
    "# I used package identifier 'popular'\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "macro-shepherd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144000, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"pickle/balanced_nov2_tweets.pick\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "domestic-bridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'date', 'time', 'user_id', 'username', 'tweet', 'hashtags',\n",
       "       'trump', 'biden'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "patent-grill",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102600"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.username.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "advanced-obligation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>trump</th>\n",
       "      <th>biden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67365</th>\n",
       "      <td>1323394849429770247</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>22:41:34</td>\n",
       "      <td>847891925394960384</td>\n",
       "      <td>boycevida</td>\n",
       "      <td>@SUBRATA30016572 @JoeBiden  https://t.co/NxBdVNalsR</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71795</th>\n",
       "      <td>1323393507692814336</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>22:36:14</td>\n",
       "      <td>979513121541967873</td>\n",
       "      <td>lindakost53</td>\n",
       "      <td>Why Is Australia Reporting This Biden Child Porn Story, But  The Media In America Hasn’t Reported It❓❓❓ Joe &amp;amp; Hunter Biden Need To Be Investigated For Child Porn &amp;amp; Sexual Assault Of Minors‼️</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103198</th>\n",
       "      <td>1323383725716111363</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>21:57:22</td>\n",
       "      <td>1203560977746276352</td>\n",
       "      <td>changed100times</td>\n",
       "      <td>@JoeBiden @KamalaHarris Where's Hunter Biden? can anyone tell me pleaseeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id       date      time              user_id  \\\n",
       "67365   1323394849429770247 2020-11-02  22:41:34   847891925394960384   \n",
       "71795   1323393507692814336 2020-11-02  22:36:14   979513121541967873   \n",
       "103198  1323383725716111363 2020-11-02  21:57:22  1203560977746276352   \n",
       "\n",
       "               username  \\\n",
       "67365         boycevida   \n",
       "71795       lindakost53   \n",
       "103198  changed100times   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                  tweet  \\\n",
       "67365                                                                                                                                                                                                                                                               @SUBRATA30016572 @JoeBiden  https://t.co/NxBdVNalsR   \n",
       "71795                                                                                                            Why Is Australia Reporting This Biden Child Porn Story, But  The Media In America Hasn’t Reported It❓❓❓ Joe &amp; Hunter Biden Need To Be Investigated For Child Porn &amp; Sexual Assault Of Minors‼️   \n",
       "103198  @JoeBiden @KamalaHarris Where's Hunter Biden? can anyone tell me pleaseeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee   \n",
       "\n",
       "       hashtags  trump  biden  \n",
       "67365        []  False   True  \n",
       "71795        []  False   True  \n",
       "103198       []  False   True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-muscle",
   "metadata": {},
   "source": [
    "Now let's create a subset, containing the same amount of Trump tweets as Biden tweets. We will exclude tweets that mention both candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "regional-mainland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trump</th>\n",
       "      <th>biden</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_id</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181142</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>2820503362</td>\n",
       "      <td>All these articles showing that Biden is in the lead.... IGNORE THAT AND STILL GO VOTE. All of these maps showing information that may or may not be correct won’t matter on Election Day. Hillary was also in the lead last election, just do your part.   ⁽ᶠᵘᶜᵏ ᵗʳᵘᵐᵖ⁾</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>1312487180258820096</td>\n",
       "      <td>@FoxNews Lady Gaga’s a nobody. Can’t figure out her own life and can’t even see nobody can help Biden. He’s out of the game from the day he gets in the game.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>2335763630</td>\n",
       "      <td>@The_Grupp “It is purely a fortuity that this isn’t one of the great mass casualty events in American history,” Ron Klain, who was Biden’s chief of staff at the time, said of H1N1 in 2019.”   https://t.co/Umi317supK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        trump  biden hashtags              user_id  \\\n",
       "181142  False   True       []           2820503362   \n",
       "0       False   True       []  1312487180258820096   \n",
       "4       False   True       []           2335763630   \n",
       "\n",
       "                                                                                                                                                                                                                                                                        original  \n",
       "181142  All these articles showing that Biden is in the lead.... IGNORE THAT AND STILL GO VOTE. All of these maps showing information that may or may not be correct won’t matter on Election Day. Hillary was also in the lead last election, just do your part.   ⁽ᶠᵘᶜᵏ ᵗʳᵘᵐᵖ⁾  \n",
       "0                                                                                                                  @FoxNews Lady Gaga’s a nobody. Can’t figure out her own life and can’t even see nobody can help Biden. He’s out of the game from the day he gets in the game.  \n",
       "4                                                        @The_Grupp “It is purely a fortuity that this isn’t one of the great mass casualty events in American history,” Ron Klain, who was Biden’s chief of staff at the time, said of H1N1 in 2019.”   https://t.co/Umi317supK  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only necessary columns\n",
    "data = df.loc[:,['tweet', 'trump', 'biden', 'hashtags', 'user_id']]\n",
    "data['original'] = data.tweet\n",
    "data.drop(columns='tweet', inplace=True)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-highway",
   "metadata": {},
   "source": [
    "## Pre-Processing Pipeline\n",
    "\n",
    "Now it's time to tokenize our tweets. Here are our pre-processing steps:\n",
    "* Remove URLs\n",
    "* Remove Twitter handles\n",
    "* Remove numbers\n",
    "* Convert to lowercase\n",
    "* Remove punctuation\n",
    "* Remove repeated letters so spell check will work ('aaaaand' -> 'aand')\n",
    "* Remove non-English words\n",
    "* Remove stop words\n",
    "\n",
    "Since we're working with so many different words, I've chosen to use **lemmatization** instead of stemming for two reasons:\n",
    "1. Lemmatization accurately reduces words to true meaning\n",
    "2. Inxreased word reduction (handles synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing these before putting them in func\n",
    "\n",
    "\n",
    "tweet = 'this is ! SAMPLE text...    @joebiden @donaldtrump #2020electionusa #2020ELECTIONUSA'\n",
    "\n",
    "# remove urls\n",
    "tweet = re.sub(r\"https?:\\/\\/\\S+\", \"\", tweet)\n",
    "# remove twitter handles\n",
    "tweet = re.sub(r\"@[\\d\\w_]+\", \"\", tweet)\n",
    "# remove numbers\n",
    "tweet = re.sub('\\w*\\d\\w*', ' ', tweet)\n",
    "# convert to lowercase\n",
    "tweet = re.sub('[%s]'.format(re.escape(string.punctuation)), ' ', tweet.lower())\n",
    "# remove repeated letters so spell check will work (ex: 'aaaand' --> 'aand')\n",
    "tweet = re.sub(r\"([a-z])\\1{2,}\", r'\\1', tweet)\n",
    "# replace consecutive spaces with one\n",
    "tweet = ' '.join(tweet.split())\n",
    "\n",
    "tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "interracial-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autocorrect import Speller # TOO SLOW...TRY PYSPELLCHECKER\n",
    "def tweet_tokenize(tweet, more_stop=None, more_words=None):\n",
    "    \"\"\"Get all of the tokens in a set of tweets.\n",
    "    \n",
    "    Parameters:\n",
    "        - tweets (Series, required)\n",
    "        \n",
    "        - more_stop (List, optional): additional stop words to exclude\n",
    "        \n",
    "        - more_words (List, optional): additional words to INCLUDE in dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    # pre-processing pipeline\n",
    "    \n",
    "    # remove urls\n",
    "    tweet = re.sub(r\"https?:\\/\\/\\S+\", \"\", tweet)\n",
    "    # remove numbers\n",
    "    tweet = re.sub('\\w*\\d\\w*', ' ', tweet)\n",
    "    # remove punctuation\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    # convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # and other popular campaign phrases\n",
    "    tweet = re.sub(r\"make america great again\", \"maga\", tweet)\n",
    "    tweet = re.sub(r\"makeamericagreatagain\", \"maga\", tweet)\n",
    "    tweet = re.sub(r\"sleepyjoe\", \"sleepy joe\", tweet)\n",
    "    tweet = re.sub(r\"sleepyjoebiden\", \"sleepy joe biden\", tweet)\n",
    "    \n",
    "    # remove spaces in candidate names\n",
    "    tweet = re.sub(r\"joebiden\", \"joe_biden\", tweet)\n",
    "    tweet = re.sub(r\"kamalaharris\", \"kamala_harris\", tweet)\n",
    "    tweet = re.sub(r\"donaldtrump\", \"donald_trump\", tweet)\n",
    "    tweet = re.sub(r\"mikepence\", \"mike_pence\", tweet)\n",
    "    tweet = re.sub(r\"joe biden\", \"joe_biden\", tweet)\n",
    "    tweet = re.sub(r\"kamala harris\", \"kamala_harris\", tweet)\n",
    "    tweet = re.sub(r\"donald trump\", \"donald_trump\", tweet)\n",
    "    tweet = re.sub(r\"mike pence\", \"mike_pence\", tweet)\n",
    "    # replace 'biden' with 'joebiden' (do for all candidates)\n",
    "    tweet = re.sub(r\"\\bbiden\\b\", \"joe_biden\", tweet)\n",
    "    tweet = re.sub(r\"\\bpence\\b\", \"mike_pence\", tweet)\n",
    "    tweet = re.sub(r\"\\bharris\\b\", \"kamala_harris\", tweet)\n",
    "    tweet = re.sub(r\"\\btrump\\b\", \"donald_trump\", tweet)\n",
    "    \n",
    "    # remove repeated letters so spell check will work (ex: 'aaaand' --> 'aand')\n",
    "    tweet = re.sub(r\"([a-z])\\1{2,}\", r'\\1', tweet)\n",
    "    # replace consecutive spaces with one\n",
    "    tweet = ' '.join(tweet.split())\n",
    "    \n",
    "    more_words = ['trump', 'biden', 'maga', 'bidenharris', \n",
    "                  'kamala', 'pence', 'harris', 'mike',\n",
    "                  'bidenharris2020', 'trumppence',\n",
    "                  'trumppence2020', 'usa', 'election2020',\n",
    "                  'ivoted', 'joe_biden', 'realdonaldtrump',\n",
    "                  'donald_trump', 'sleepy_joe',\n",
    "                  'mike_pence', 'kamala_harris']\n",
    "    \n",
    "    dictionary = list(words.words()) + more_words\n",
    "    dictionary = set(dictionary)\n",
    "    \n",
    "    twt = TweetTokenizer()\n",
    "    tokens = [token for token in twt.tokenize(tweet) if token in dictionary]\n",
    "    \n",
    "    # initiate stop word removal and lemmatization    \n",
    "    more_stop = ['fxhedg','fyck','fy','fxxking','give','go',\n",
    "                 'going','gonna','get','one','de','la','el','en','un','ha',\n",
    "                 'would','dont','know','time','think','want','via','dont']\n",
    "    \n",
    "    stop_words = list(stopwords.words('english')) + more_stop\n",
    "    stop = stop_words\n",
    "    stop = set(stop)\n",
    "    \n",
    "    lemm = WordNetLemmatizer()\n",
    "    \n",
    "    # implement lemmatization and stop word removal\n",
    "    tokens = [lemm.lemmatize(token) for token in tokens\n",
    "              if token.lower() not in stop]\n",
    "#     spell = Speller(lang='en')\n",
    "#     tokens = [spell(t) for t in tokens]\n",
    "\n",
    "    combined_tokens = ' '.join(tokens)\n",
    "\n",
    "    return combined_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'] = data['original'].map(tweet_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(\"pickle/n2_tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_pickle(\"pickle/tweets_df_5000tw.pick\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-heart",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=0.02, max_df=0.10)\n",
    "doc_words = cv.fit_transform(data.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-reliance",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(n_components=5)\n",
    "doc_topic = nmf_model.fit_transform(doc_words)\n",
    "print(f\"Shape: {doc_topic.shape}\")\n",
    "print(f\"Number of iterations used: {nmf_model.n_iter_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-matter",
   "metadata": {},
   "source": [
    "From lecture: The **doc_topic** matrix shows us the documents we started with, and how each document is made up of the 2 resulting topics. We don't know yet what the topics are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = nmf_model.components_\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word.argsort()[:,-1:-7:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-commonwealth",
   "metadata": {},
   "source": [
    "From lecture: The **topic_word** matrix shows us the 2 resulting topics, and the terms that are associated with each topic. By looking at the words below, we an figure out what the topics are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = cv.get_feature_names()\n",
    "t = nmf_model.components_.argsort(axis=1)[:,-1:-7:-1]\n",
    "topic_words = [[words[e] for e in l] for l in t]\n",
    "topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-channel",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=2)\n",
    "doc_topic = lda_model.fit_transform(doc_words)\n",
    "doc_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = cv.get_feature_names()\n",
    "t = lda_model.components_.argsort(axis=1)[:,-1:-7:-1]\n",
    "topic_words = [[words[e] for e in l] for l in t]\n",
    "topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-donna",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-kingston",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid_obj = SentimentIntensityAnalyzer()\n",
    "sentiment = []\n",
    "for text in data.tweet:\n",
    "    sentiment.append(sid_obj.polarity_scores(text))\n",
    "    \n",
    "pd.concat([data,pd.DataFrame(sentiment)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-dollar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-seller",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-binary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-louis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
