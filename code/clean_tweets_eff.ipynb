{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "authorized-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "integral-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 40)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "inner-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must uncomment & run the first time to DOWNLOAD NLTK data\n",
    "# I used package identifier 'popular'\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "macro-shepherd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96000, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle(\"pickle/balanced_nov2_tweets.pick\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regional-mainland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>trump</th>\n",
       "      <th>biden</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181142</th>\n",
       "      <td>1323379284434669568</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>21:39:43</td>\n",
       "      <td>2820503362</td>\n",
       "      <td>artistacriseida</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>All these articles showing that Biden is in the lead.... IGNORE THAT AND STILL GO VOTE. All of these maps showing information that may or may not be correct won‚Äôt matter on Election Day. Hillary was also in the lead last election, just do your part.   ‚ÅΩ·∂†·µò·∂ú·µè ·µó ≥·µò·µê·µñ‚Åæ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1323414585995526144</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>23:59:59</td>\n",
       "      <td>1312487180258820096</td>\n",
       "      <td>annapieters17</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>@FoxNews Lady Gaga‚Äôs a nobody. Can‚Äôt figure out her own life and can‚Äôt even see nobody can help Biden. He‚Äôs out of the game from the day he gets in the game.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1323414585232293888</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>23:59:59</td>\n",
       "      <td>2335763630</td>\n",
       "      <td>kylechwatt</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>@The_Grupp ‚ÄúIt is purely a fortuity that this isn‚Äôt one of the great mass casualty events in American history,‚Äù Ron Klain, who was Biden‚Äôs chief of staff at the time, said of H1N1 in 2019.‚Äù   https://t.co/Umi317supK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id       date      time              user_id  \\\n",
       "181142  1323379284434669568 2020-11-02  21:39:43           2820503362   \n",
       "0       1323414585995526144 2020-11-02  23:59:59  1312487180258820096   \n",
       "4       1323414585232293888 2020-11-02  23:59:59           2335763630   \n",
       "\n",
       "               username hashtags  trump  biden  \\\n",
       "181142  artistacriseida       []  False   True   \n",
       "0         annapieters17       []  False   True   \n",
       "4            kylechwatt       []  False   True   \n",
       "\n",
       "                                                                                                                                                                                                                                                                        original  \n",
       "181142  All these articles showing that Biden is in the lead.... IGNORE THAT AND STILL GO VOTE. All of these maps showing information that may or may not be correct won‚Äôt matter on Election Day. Hillary was also in the lead last election, just do your part.   ‚ÅΩ·∂†·µò·∂ú·µè ·µó ≥·µò·µê·µñ‚Åæ  \n",
       "0                                                                                                                  @FoxNews Lady Gaga‚Äôs a nobody. Can‚Äôt figure out her own life and can‚Äôt even see nobody can help Biden. He‚Äôs out of the game from the day he gets in the game.  \n",
       "4                                                        @The_Grupp ‚ÄúIt is purely a fortuity that this isn‚Äôt one of the great mass casualty events in American history,‚Äù Ron Klain, who was Biden‚Äôs chief of staff at the time, said of H1N1 in 2019.‚Äù   https://t.co/Umi317supK  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only necessary columns\n",
    "data['original'] = data.tweet\n",
    "data.drop(columns='tweet', inplace=True)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-highway",
   "metadata": {},
   "source": [
    "## Pre-Processing Pipeline\n",
    "\n",
    "Now it's time to tokenize our tweets. Here are our pre-processing steps:\n",
    "* Remove URLs\n",
    "* Remove Twitter handles\n",
    "* Remove numbers\n",
    "* Convert to lowercase\n",
    "* Remove punctuation\n",
    "* Remove repeated letters so spell check will work ('aaaaand' -> 'aand')\n",
    "* Remove non-English words\n",
    "* Remove stop words\n",
    "\n",
    "Since we're working with so many different words, I've chosen to use **lemmatization** instead of stemming for two reasons:\n",
    "1. Lemmatization accurately reduces words to true meaning\n",
    "2. Inxreased word reduction (handles synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "motivated-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download() # must run first time (download 'popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stock-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom word dictionaries\n",
    "from more_words import more_words as custom_words\n",
    "from stop_words import stop_words as custom_stop_words\n",
    "from bigrams import bigrams\n",
    "\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import words, stopwords\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    \"\"\"Pre-processing pipeline.\"\"\"\n",
    "    \n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"u\\.s\\. \", \"usa\", tweet)\n",
    "    tweet = re.sub(r\"https?:\\/\\/\\S+\", \"\", tweet)\n",
    "    tweet = re.sub(r'\\w*\\d\\w*', ' ', tweet)\n",
    "    tweet = re.sub(r'\\.{2,6}', ' ', tweet)\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    tweet = re.sub(r\"([a-z])\\1{2,5}\", r'\\1', tweet)\n",
    "    tweet = ' '.join(tweet.split())\n",
    "    \n",
    "    for old, new in bigrams:\n",
    "        tweet = re.sub(old, new, tweet) # ex: ('white house', 'whitehouse')\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def tokenize(tweet, all_words, stop_words):\n",
    "    \"\"\"\n",
    "    Returns all of the tokens in a cleaned tweet\n",
    "    Parameters:\n",
    "        - tweet (Series, required)\n",
    "        - stop_words (set, required)\n",
    "        - all_words (set, required) \n",
    "        \n",
    "    Note:\n",
    "        - any words not included in all_words here will be\n",
    "          removed from tokens (including bigrams)\n",
    "    \"\"\"\n",
    "    \n",
    "    tweet = clean_tweet(tweet)\n",
    "    \n",
    "    twt = TweetTokenizer()    \n",
    "    lemm = WordNetLemmatizer()  \n",
    "    # lemmatize tokens & remove stop words\n",
    "    tokens = [lemm.lemmatize(token) for token in twt.tokenize(tweet) if token not in stop_words]\n",
    "    # only include words that are in our customized list of words\n",
    "    tokens = [token for token in tokens if token in all_words]\n",
    "    combined_tokens = ' '.join(tokens)\n",
    "\n",
    "    return combined_tokens\n",
    "\n",
    "\n",
    "def clean_and_tokenize(original_tweets):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        original_tweets (required, pd.Series)\n",
    "    \n",
    "    Returns:\n",
    "        cleaned, tokenized tweets (np.array)\n",
    "    \"\"\"\n",
    "    tweet_array = original_tweets.to_numpy()\n",
    "\n",
    "    all_words = list(words.words('en')) + custom_words\n",
    "    stop_words = set(list(stopwords.words('english')) + custom_stop_words)\n",
    "    \n",
    "    for _, new in bigrams:\n",
    "        all_words.append(new)\n",
    "\n",
    "    all_words = set(all_words)\n",
    "    print(\"num words: \", len(all_words))\n",
    "    print(\"num stop words: \", len(stop_words))\n",
    "    \n",
    "    clean_tweets = [tokenize(tweet, all_words, stop_words) for tweet in tweet_array]\n",
    "    \n",
    "    return clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "progressive-egypt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num words:  236014\n",
      "num stop words:  346\n",
      "CPU times: user 30 s, sys: 27.9 ms, total: 30 s\n",
      "Wall time: 30 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(96000, 11)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data['tweet'] = clean_and_tokenize(data.original)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "musical-yacht",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74248, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['num_tokens'] = data['tweet'].str.count(' ') + 1\n",
    "\n",
    "mask = data['num_tokens'] >= 5\n",
    "data_minlen = data[mask]\n",
    "data_minlen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "clear-digit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Trump Tweets 38402\n",
      "\n",
      " Biden Tweets: 35846\n"
     ]
    }
   ],
   "source": [
    "mask = (data_minlen.trump == 0) & (data_minlen.biden == 1)\n",
    "biden = data_minlen[mask]\n",
    "mask = (data_minlen.trump ==1) & (data_minlen.biden == 0)\n",
    "trump = data_minlen[mask]\n",
    "\n",
    "print(f\"\\n Trump Tweets {len(trump)}\\n\\n Biden Tweets: {len(biden)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-administration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "casual-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(\"pickle/n2_tokenized_eff.pick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "egyptian-surface",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70143</th>\n",
       "      <td>@Rational1414 @NRA Me &amp;amp; my wife and my children all want Trump for 4 more !!! And horus paine quit drinking the kool aid !!!</td>\n",
       "      <td>wife child donaldtrump quit drinking aid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66896</th>\n",
       "      <td>√â mto bom ver mina moderninha na internet querendo revolucionar os EUA se posicionando contra o Trump, como se fosse fazer diferen√ßa</td>\n",
       "      <td>bom mina o contra donaldtrump fosse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47630</th>\n",
       "      <td>#NPO. Mensen zijn onpeilbaar en nog iets.  ben t ff kwijt. Moet je nagaan hoe verheven zij zichzelf vind. Trump wint.  https://t.co/NnFs51yEUs</td>\n",
       "      <td>nog ben hoe donaldtrump wint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4758</th>\n",
       "      <td>@SteadyasweDodo I know!!! I haven‚Äôt slept in days. I wake up every couple of hours since two weeks ago...and yes...with Trump nightmares.</td>\n",
       "      <td>slept day wake couple hour since two week ago donaldtrump nightmare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29262</th>\n",
       "      <td>tomorrow could either be the icing on the cake of 2020 or the best day of 2020. you choose. #voteblue  - - -  @JoeBiden , we got thisüí™</td>\n",
       "      <td>either icing cake best choose voteblue joebiden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73803</th>\n",
       "      <td>'Appalling, disturbing and criminal,' Galvin says of Trump's comments about voting  https://t.co/w22yX3n72u via @Yahoo yes, it's criminal..................</td>\n",
       "      <td>appalling disturbing criminal say trump comment voting yahoo criminal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19895</th>\n",
       "      <td>@KamalaHarris @USPS @JoeBiden should Stop with the phony narrative!! Seems like he pull the Scranton card only when it‚Äôs convenient for him, what have he done for Scranton over 47 years.  BTW, his Eagles jacket is a UD Blue Hens jacket #MAGA #MAGA2020LandslideVictory #2020Election #CorruptJoeBiden  https://t.co/IwBVaWWedR</td>\n",
       "      <td>kamalaharris joebiden stop phony narrative pull card convenient year eagle jacket blue hen jacket maga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16581</th>\n",
       "      <td>@Third_Witness @Jarmer16 @music_jeb @certifiedpoints @JoeBiden Lmao the green line is what you're saying caused the rise.  https://t.co/JSpgGA3Mcm</td>\n",
       "      <td>joebiden green line rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77470</th>\n",
       "      <td>Joe is good at lies lies and more lies. Trump took DJIA from 16,000 to over 30,000 before pandemic. Obama took DJIA from 12,000 to 16,000 in 8 years. You and your moronic moments. Joe you don't give Trump and the pandemic enough credit. You just want to play the blame game.</td>\n",
       "      <td>joebiden lie lie lie donaldtrump took pandemic barackobama took year moronic moment joebiden give donaldtrump pandemic enough credit play blame game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61683</th>\n",
       "      <td>@realDonaldTrump \"If the NYT story is 'fake news,' Trump can prove it by releasing just one page of his 2016 &amp;amp; '17 returns, that critical last page of Form 1040 that includes his signature and total tax paid. One page [and] he could thoroughly humiliate the media. He won't, and we all know why.\"</td>\n",
       "      <td>realdonaldtrump story fakenews donaldtrump prove page return critical page form signature tax page thoroughly humiliate medium wont</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                  original  \\\n",
       "70143                                                                                                                                                                                                     @Rational1414 @NRA Me &amp; my wife and my children all want Trump for 4 more !!! And horus paine quit drinking the kool aid !!!   \n",
       "66896                                                                                                                                                                                                 √â mto bom ver mina moderninha na internet querendo revolucionar os EUA se posicionando contra o Trump, como se fosse fazer diferen√ßa   \n",
       "47630                                                                                                                                                                                       #NPO. Mensen zijn onpeilbaar en nog iets.  ben t ff kwijt. Moet je nagaan hoe verheven zij zichzelf vind. Trump wint.  https://t.co/NnFs51yEUs   \n",
       "4758                                                                                                                                                                                             @SteadyasweDodo I know!!! I haven‚Äôt slept in days. I wake up every couple of hours since two weeks ago...and yes...with Trump nightmares.   \n",
       "29262                                                                                                                                                                                               tomorrow could either be the icing on the cake of 2020 or the best day of 2020. you choose. #voteblue  - - -  @JoeBiden , we got thisüí™   \n",
       "...                                                                                                                                                                                                                                                                                                                                    ...   \n",
       "73803                                                                                                                                                                          'Appalling, disturbing and criminal,' Galvin says of Trump's comments about voting  https://t.co/w22yX3n72u via @Yahoo yes, it's criminal..................   \n",
       "19895  @KamalaHarris @USPS @JoeBiden should Stop with the phony narrative!! Seems like he pull the Scranton card only when it‚Äôs convenient for him, what have he done for Scranton over 47 years.  BTW, his Eagles jacket is a UD Blue Hens jacket #MAGA #MAGA2020LandslideVictory #2020Election #CorruptJoeBiden  https://t.co/IwBVaWWedR   \n",
       "16581                                                                                                                                                                                   @Third_Witness @Jarmer16 @music_jeb @certifiedpoints @JoeBiden Lmao the green line is what you're saying caused the rise.  https://t.co/JSpgGA3Mcm   \n",
       "77470                                                   Joe is good at lies lies and more lies. Trump took DJIA from 16,000 to over 30,000 before pandemic. Obama took DJIA from 12,000 to 16,000 in 8 years. You and your moronic moments. Joe you don't give Trump and the pandemic enough credit. You just want to play the blame game.   \n",
       "61683                         @realDonaldTrump \"If the NYT story is 'fake news,' Trump can prove it by releasing just one page of his 2016 &amp; '17 returns, that critical last page of Form 1040 that includes his signature and total tax paid. One page [and] he could thoroughly humiliate the media. He won't, and we all know why.\"   \n",
       "\n",
       "                                                                                                                                                      tweet  \n",
       "70143                                                                                                              wife child donaldtrump quit drinking aid  \n",
       "66896                                                                                                                   bom mina o contra donaldtrump fosse  \n",
       "47630                                                                                                                          nog ben hoe donaldtrump wint  \n",
       "4758                                                                                    slept day wake couple hour since two week ago donaldtrump nightmare  \n",
       "29262                                                                                                       either icing cake best choose voteblue joebiden  \n",
       "...                                                                                                                                                     ...  \n",
       "73803                                                                                 appalling disturbing criminal say trump comment voting yahoo criminal  \n",
       "19895                                                kamalaharris joebiden stop phony narrative pull card convenient year eagle jacket blue hen jacket maga  \n",
       "16581                                                                                                                              joebiden green line rise  \n",
       "77470  joebiden lie lie lie donaldtrump took pandemic barackobama took year moronic moment joebiden give donaldtrump pandemic enough credit play blame game  \n",
       "61683                   realdonaldtrump story fakenews donaldtrump prove page return critical page form signature tax page thoroughly humiliate medium wont  \n",
       "\n",
       "[15 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['original', 'tweet']].sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "geological-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_pickle(\"pickle/tweets_df_5000tw.pick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "confused-competition",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweet_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e58bd57c10c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tweet tokenize me please mr. biden helloaskldjalksfj I  pence    voting rights am asking for a favor continuous breakdown American Americans'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtweet_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tweet_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "sample = 'tweet tokenize me please mr. biden helloaskldjalksfj I  pence    voting rights am asking for a favor continuous breakdown American Americans'\n",
    "\n",
    "tweet_tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-invalid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-companion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_min4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_min4.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "lemm.lemmatize('organizations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_pickle(\"pickle/n2_tokenized_min4.pick\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
