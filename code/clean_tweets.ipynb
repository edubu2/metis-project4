{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords = spacy.lang('en').stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-holiday",
   "metadata": {},
   "source": [
    "**Kelsey 1-1**\n",
    "\n",
    "- cleaning\n",
    "    - preprocessing until comfortable with words\n",
    "   \n",
    "- sentiment analysis on all tweets\n",
    "    - don't need to do any splitting at this stage\n",
    "    - TextBlob & VaderSentiment first, spacy if the results aren't as expected\n",
    "    \n",
    "- topic modeling\n",
    "    - decide: use all tweets (all topics) at once\n",
    "        - start here\n",
    "        - then can use these as features in the dataFrame and do splitting here\n",
    "    - or: split to trump/biden - then bot/not bot for each\n",
    "    - point here is there are multiple ways to split it\n",
    "        - no right answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 40)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must uncomment & run the first time to DOWNLOAD NLTK data\n",
    "# I used package identifier 'popular'\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"pickle/balanced_nov2_tweets.pick\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.username.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-muscle",
   "metadata": {},
   "source": [
    "Now let's create a subset, containing the same amount of Trump tweets as Biden tweets. We will exclude tweets that mention both candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only necessary columns\n",
    "data['original'] = data.tweet\n",
    "data.drop(columns='tweet', inplace=True)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-wallet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "welsh-highway",
   "metadata": {},
   "source": [
    "## Pre-Processing Pipeline\n",
    "\n",
    "Now it's time to tokenize our tweets. Here are our pre-processing steps:\n",
    "* Remove URLs\n",
    "* Remove Twitter handles\n",
    "* Remove numbers\n",
    "* Convert to lowercase\n",
    "* Remove punctuation\n",
    "* Remove repeated letters so spell check will work ('aaaaand' -> 'aand')\n",
    "* Remove non-English words\n",
    "* Remove stop words\n",
    "\n",
    "Since we're working with so many different words, I've chosen to use **lemmatization** instead of stemming for two reasons:\n",
    "1. Lemmatization accurately reduces words to true meaning\n",
    "2. Inxreased word reduction (handles synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing these before putting them in func\n",
    "\n",
    "\n",
    "tweet = 'this is ! SAMPLE text...  blm united states of america U.S.A usa U.S.  @joebiden @donaldtrump #2020electionusa #2020ELECTIONUSA'\n",
    "\n",
    "# pre-processing pipeline\n",
    "\n",
    "# pre-processing pipeline\n",
    "\n",
    "# convert to lowercase\n",
    "tweet = tweet.lower()\n",
    "\n",
    "# convert 'U.S.' --> 'usa'\n",
    "tweet = re.sub(r\"u\\.s\\. \", \"usa\", tweet)\n",
    "# remove urls\n",
    "tweet = re.sub(r\"https?:\\/\\/\\S+\", \"\", tweet)\n",
    "# remove numbers\n",
    "tweet = re.sub('\\w*\\d\\w*', ' ', tweet)\n",
    "# replace '...' with ' '\n",
    "tweet = re.sub('\\.{2,6}', ' ', tweet)\n",
    "\n",
    "# remove punctuation\n",
    "tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# remove repeated letters so spell check will work (ex: 'aaaand' --> 'aand')\n",
    "tweet = re.sub(r\"([a-z])\\1{2,5}\", r'\\1', tweet)\n",
    "# replace consecutive spaces with one\n",
    "tweet = ' '.join(tweet.split())\n",
    "\n",
    "# remove spaces in candidate names\n",
    "tweet = re.sub(r\"joebiden\", \"joe_biden\", tweet)\n",
    "tweet = re.sub(r\"kamalaharris\", \"kamala_harris\", tweet)\n",
    "tweet = re.sub(r\"president trump\", \"donald_trump\", tweet)\n",
    "tweet = re.sub(r\"president donald trump\", \"donald_trump\", tweet)\n",
    "tweet = re.sub(r\"vice president mike pence\", \"mike_pence\", tweet)\n",
    "tweet = re.sub(r\"vice president pence\", \"mike_pence\", tweet)\n",
    "tweet = re.sub(r\"vice president kamala harris\", \"kamala_harris\", tweet)\n",
    "tweet = re.sub(r\"vice president harris\", \"kamala_harris\", tweet)\n",
    "tweet = re.sub(r\"democratic presidential nominee\", \"democratic_presidential_nominee\", tweet)\n",
    "tweet = re.sub(r\"republican presidential nominee\", \"republican_presidential_nominee\", tweet)\n",
    "tweet = re.sub(r\"red state\", \"red_state\", tweet)\n",
    "tweet = re.sub(r\"blue state\", \"blue_state\", tweet)\n",
    "tweet = re.sub(r\"absentee ballot\", \"absentee_ballot\", tweet)\n",
    "tweet = re.sub(r\"voting rights\", \"voting_rights\", tweet)\n",
    "\n",
    "tweet = re.sub(r\"donaldtrump\", \"donald_trump\", tweet)\n",
    "tweet = re.sub(r\"mikepence\", \"mike_pence\", tweet)\n",
    "tweet = re.sub(r\"joe biden\", \"joe_biden\", tweet)\n",
    "tweet = re.sub(r\"kamala harris\", \"kamala_harris\", tweet)\n",
    "tweet = re.sub(r\"donald trump\", \"donald_trump\", tweet)\n",
    "tweet = re.sub(r\"mike pence\", \"mike_pence\", tweet)\n",
    "tweet = re.sub(r\"nancy pelosi\", \"nancy_pelosi\", tweet)\n",
    "tweet = re.sub(r\"mitch mcconnell\", \"mitch_mcconnell\", tweet)\n",
    "\n",
    "# replace 'biden' with 'joebiden' (do for all candidates)\n",
    "tweet = re.sub(r\"\\bbiden\\b\", \"joe_biden\", tweet)\n",
    "tweet = re.sub(r\"\\bpence\\b\", \"mike_pence\", tweet)\n",
    "tweet = re.sub(r\"\\bharris\\b\", \"kamala_harris\", tweet)\n",
    "tweet = re.sub(r\"\\btrump\\b\", \"donald_trump\", tweet)\n",
    "# other replacements\n",
    "tweet = re.sub(r\"united states of america\", \"usa\", tweet)\n",
    "tweet = re.sub(r\"pro life\", \"pro_life\", tweet)\n",
    "tweet = re.sub(r\"pro choice\", \"pro_choice\", tweet)\n",
    "tweet = re.sub(r\"black lives matter\", \"black_lives_matter\", tweet)\n",
    "tweet = re.sub(r\"blm\", \"black_lives_matter\", tweet)\n",
    "tweet = re.sub(r\"blacklivesmatter\", \"black_lives_matter\", tweet)\n",
    "tweet = re.sub(r\"mailin ballots\", \"mail_in_ballots\", tweet)\n",
    "tweet = re.sub(r\"mailin\", \"mail_in_ballots\", tweet)\n",
    "tweet = re.sub(r\"united states\", \"usa\", tweet)\n",
    "tweet = re.sub(r\"attorney general\", \"attorney_general\", tweet)\n",
    "tweet = re.sub(r\"white house\", \"white_house\", tweet)\n",
    "tweet = re.sub(r\"make america great again\", \"maga\", tweet)\n",
    "tweet = re.sub(r\"makeamericagreatagain\", \"maga\", tweet)\n",
    "tweet = re.sub(r\"election fraud\", \"election fraud\", tweet)\n",
    "tweet = re.sub(r\"sleepy joe biden\", \"sleepy_joe\", tweet)\n",
    "tweet = re.sub(r\"sleepy joe\", \"sleepy_joe\", tweet)\n",
    "tweet = re.sub(r\"presidential election\", \"election\", tweet)\n",
    "tweet = re.sub(r\"running mate\", \"running_mate\", tweet)\n",
    "tweet = re.sub(r\"voting machine\", \"voting_machine\", tweet)\n",
    "tweet = re.sub(r\"cast your ballot\", \"cast_your_ballot\", tweet)\n",
    "tweet = re.sub(r\"foreign policy\", \"foreign_policy\", tweet)\n",
    "tweet = re.sub(r\"election day\", \"election_day\", tweet)\n",
    "tweet = re.sub(r\"voting booth\", \"voting_booth\", tweet)\n",
    "tweet = re.sub(r\"radical left\", \"radical_left\", tweet)\n",
    "tweet = re.sub(r\"free speech\", \"free_speech\", tweet)\n",
    "tweet = re.sub(r\"first amendment\", \"first_amendment\", tweet)\n",
    "tweet = re.sub(r\"racial injustice\", \"racial_injustice\", tweet)\n",
    "tweet = re.sub(r\"social inequality\", \"social_inequality\", tweet)\n",
    "tweet = re.sub(r\"russian interference\", \"russian_inteference\", tweet)\n",
    "tweet = re.sub(r\"electoral college\", \"electoral_college\", tweet)\n",
    "tweet = re.sub(r\"right wing\", \"right_wing\", tweet)\n",
    "tweet = re.sub(r\"left wing\", \"left_wing\", tweet)\n",
    "tweet = re.sub(r\"far right\", \"far_right\", tweet)\n",
    "tweet = re.sub(r\"far left\", \"far_left\", tweet)\n",
    "tweet = re.sub(r\"conspiracy theory\", \"conspiracy_theory\", tweet)\n",
    "tweet = re.sub(r\"domestic terrorism\", \"domestic_terrorism\", tweet)\n",
    "tweet = re.sub(r\"vice president\", \"vice president\", tweet)\n",
    "\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords, words\n",
    "# print(stopwords.words('english'))\n",
    "print(list(words.words('en'))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autocorrect import Speller # TOO SLOW...TRY PYSPELLCHECKER\n",
    "# custom word dictionaries\n",
    "from more_words import more_words as custom_words\n",
    "from stop_words import stop_words as custom_stop_words\n",
    "from multi_words import multi_words\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import words, stopwords\n",
    "\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    # pre-processing pipeline\n",
    "    \n",
    "    # convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # convert 'U.S.' --> 'usa'\n",
    "    tweet = re.sub(r\"u\\.s\\. \", \"usa\", tweet)\n",
    "    # remove urls\n",
    "    tweet = re.sub(r\"https?:\\/\\/\\S+\", \"\", tweet)\n",
    "    # remove numbers\n",
    "    tweet = re.sub('\\w*\\d\\w*', ' ', tweet)\n",
    "    # replace '...' with ' '\n",
    "    tweet = re.sub('\\.{2,6}', ' ', tweet)\n",
    "    # remove punctuation\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    # remove repeated letters so spell check will work (ex: 'aaaand' --> 'aand')\n",
    "    tweet = re.sub(r\"([a-z])\\1{2,5}\", r'\\1', tweet)\n",
    "    # replace consecutive spaces with one\n",
    "    tweet = ' '.join(tweet.split())\n",
    "    \n",
    "    # custom replacements. multiwords is a list of tuples such as ('white house', 'white_house')    \n",
    "    for old, new in multi_words:\n",
    "        tweet = re.sub(old, new, tweet)\n",
    "    return tweet\n",
    "\n",
    "def tweet_tokenize(tweet, more_stop=None, more_words=None):\n",
    "    \"\"\"\n",
    "    Get all of the tokens in a set of tweets.\n",
    "    Parameters:\n",
    "        - tweets (Series, required)\n",
    "        - more_stop (List, optional): additional stop words to exclude\n",
    "        - more_words (List, optional): additional words to INCLUDE in dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    tweet = clean_tweet(tweet)\n",
    "    stop_words = set(list(stopwords.words('english')) + custom_stop_words)\n",
    "    all_words = set(list(words.words('en')) + custom_words)\n",
    "    print(len(all_words))\n",
    "    \n",
    "    # lemmatize text\n",
    "    twt = TweetTokenizer()\n",
    "    lemm = WordNetLemmatizer()\n",
    "    \n",
    "    tokens = [lemm.lemmatize(token) for token in twt.tokenize(tweet) if token in all_words and token not in stop_words]\n",
    "#     spell = Speller(lang='en')\n",
    "#     tokens = [spell(t) for t in tokens]\n",
    "\n",
    "    combined_tokens = ' '.join(tokens)\n",
    "    \n",
    "\n",
    "\n",
    "    return combined_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "lemm.lemmatize('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data['tweet'] = data['original'].map(tweet_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(\"pickle/n2_tokenized.pick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_pickle(\"pickle/tweets_df_5000tw.pick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-binary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-louis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-europe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-failing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-conference",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
