{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "authorized-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-holiday",
   "metadata": {},
   "source": [
    "**Kelsey 1-1**\n",
    "\n",
    "- cleaning\n",
    "    - preprocessing until comfortable with words\n",
    "   \n",
    "- sentiment analysis on all tweets\n",
    "    - don't need to do any splitting at this stage\n",
    "    - TextBlob & VaderSentiment first, spacy if the results aren't as expected\n",
    "    \n",
    "- topic modeling\n",
    "    - decide: use all tweets (all topics) at once\n",
    "        - start here\n",
    "        - then can use these as features in the dataFrame and do splitting here\n",
    "    - or: split to trump/biden - then bot/not bot for each\n",
    "    - point here is there are multiple ways to split it\n",
    "        - no right answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "integral-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 40)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "inner-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must uncomment & run the first time to DOWNLOAD NLTK data\n",
    "# I used package identifier 'popular'\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "macro-shepherd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96000, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle(\"pickle/balanced_nov2_tweets.pick\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "advanced-obligation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>trump</th>\n",
       "      <th>biden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38562</th>\n",
       "      <td>1323403517688774656</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>23:16:00</td>\n",
       "      <td>1077789533473857537</td>\n",
       "      <td>rosemar95753967</td>\n",
       "      <td>POLITICO's final Election Forecast: Biden in command, Senate up for grabs  The ratings are the result of a yearlong reporting project based on conversations with dozens of strategists, operatives and pollsters.</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99851</th>\n",
       "      <td>1323384695057309696</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>22:01:13</td>\n",
       "      <td>397012977</td>\n",
       "      <td>jimmyespada</td>\n",
       "      <td>@SherylCole1 @JoeBiden @AustinYoungDems You only cancelled because you knew no one would show up...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100375</th>\n",
       "      <td>1323384547183153155</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>22:00:38</td>\n",
       "      <td>1902162757</td>\n",
       "      <td>catgrey58</td>\n",
       "      <td>Biden with Lady Gaga....oh law. The man can‚Äôt even wear a mask properly. üôÑ</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id       date      time              user_id  \\\n",
       "38562   1323403517688774656 2020-11-02  23:16:00  1077789533473857537   \n",
       "99851   1323384695057309696 2020-11-02  22:01:13            397012977   \n",
       "100375  1323384547183153155 2020-11-02  22:00:38           1902162757   \n",
       "\n",
       "               username  \\\n",
       "38562   rosemar95753967   \n",
       "99851       jimmyespada   \n",
       "100375        catgrey58   \n",
       "\n",
       "                                                                                                                                                                                                                     tweet  \\\n",
       "38562   POLITICO's final Election Forecast: Biden in command, Senate up for grabs  The ratings are the result of a yearlong reporting project based on conversations with dozens of strategists, operatives and pollsters.   \n",
       "99851                                                                                                                  @SherylCole1 @JoeBiden @AustinYoungDems You only cancelled because you knew no one would show up...   \n",
       "100375                                                                                                                                          Biden with Lady Gaga....oh law. The man can‚Äôt even wear a mask properly. üôÑ   \n",
       "\n",
       "       hashtags  trump  biden  \n",
       "38562        []  False   True  \n",
       "99851        []  False   True  \n",
       "100375       []  False   True  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-muscle",
   "metadata": {},
   "source": [
    "Now let's create a subset, containing the same amount of Trump tweets as Biden tweets. We will exclude tweets that mention both candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "regional-mainland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>trump</th>\n",
       "      <th>biden</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181142</th>\n",
       "      <td>1323379284434669568</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>21:39:43</td>\n",
       "      <td>2820503362</td>\n",
       "      <td>artistacriseida</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>All these articles showing that Biden is in the lead.... IGNORE THAT AND STILL GO VOTE. All of these maps showing information that may or may not be correct won‚Äôt matter on Election Day. Hillary was also in the lead last election, just do your part.   ‚ÅΩ·∂†·µò·∂ú·µè ·µó ≥·µò·µê·µñ‚Åæ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1323414585995526144</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>23:59:59</td>\n",
       "      <td>1312487180258820096</td>\n",
       "      <td>annapieters17</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>@FoxNews Lady Gaga‚Äôs a nobody. Can‚Äôt figure out her own life and can‚Äôt even see nobody can help Biden. He‚Äôs out of the game from the day he gets in the game.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1323414585232293888</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>23:59:59</td>\n",
       "      <td>2335763630</td>\n",
       "      <td>kylechwatt</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>@The_Grupp ‚ÄúIt is purely a fortuity that this isn‚Äôt one of the great mass casualty events in American history,‚Äù Ron Klain, who was Biden‚Äôs chief of staff at the time, said of H1N1 in 2019.‚Äù   https://t.co/Umi317supK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id       date      time              user_id  \\\n",
       "181142  1323379284434669568 2020-11-02  21:39:43           2820503362   \n",
       "0       1323414585995526144 2020-11-02  23:59:59  1312487180258820096   \n",
       "4       1323414585232293888 2020-11-02  23:59:59           2335763630   \n",
       "\n",
       "               username hashtags  trump  biden  \\\n",
       "181142  artistacriseida       []  False   True   \n",
       "0         annapieters17       []  False   True   \n",
       "4            kylechwatt       []  False   True   \n",
       "\n",
       "                                                                                                                                                                                                                                                                        original  \n",
       "181142  All these articles showing that Biden is in the lead.... IGNORE THAT AND STILL GO VOTE. All of these maps showing information that may or may not be correct won‚Äôt matter on Election Day. Hillary was also in the lead last election, just do your part.   ‚ÅΩ·∂†·µò·∂ú·µè ·µó ≥·µò·µê·µñ‚Åæ  \n",
       "0                                                                                                                  @FoxNews Lady Gaga‚Äôs a nobody. Can‚Äôt figure out her own life and can‚Äôt even see nobody can help Biden. He‚Äôs out of the game from the day he gets in the game.  \n",
       "4                                                        @The_Grupp ‚ÄúIt is purely a fortuity that this isn‚Äôt one of the great mass casualty events in American history,‚Äù Ron Klain, who was Biden‚Äôs chief of staff at the time, said of H1N1 in 2019.‚Äù   https://t.co/Umi317supK  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only necessary columns\n",
    "data['original'] = data.tweet\n",
    "data.drop(columns='tweet', inplace=True)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-highway",
   "metadata": {},
   "source": [
    "## Pre-Processing Pipeline\n",
    "\n",
    "Now it's time to tokenize our tweets. Here are our pre-processing steps:\n",
    "* Remove URLs\n",
    "* Remove Twitter handles\n",
    "* Remove numbers\n",
    "* Convert to lowercase\n",
    "* Remove punctuation\n",
    "* Remove repeated letters so spell check will work ('aaaaand' -> 'aand')\n",
    "* Remove non-English words\n",
    "* Remove stop words\n",
    "\n",
    "Since we're working with so many different words, I've chosen to use **lemmatization** instead of stemming for two reasons:\n",
    "1. Lemmatization accurately reduces words to true meaning\n",
    "2. Inxreased word reduction (handles synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "motivated-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download() # must run first time (download 'popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "operational-genre",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interracial-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autocorrect import Speller # TOO SLOW...TRY PYSPELLCHECKER\n",
    "# custom word dictionaries\n",
    "from more_words import more_words as custom_words\n",
    "from stop_words import stop_words as custom_stop_words\n",
    "from multi_words import multi_words\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import words, stopwords\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    # pre-processing pipeline\n",
    "    \n",
    "    # convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # convert 'U.S.' --> 'usa'\n",
    "    tweet = re.sub(r\"u\\.s\\. \", \"usa\", tweet)\n",
    "    # remove urls\n",
    "    tweet = re.sub(r\"https?:\\/\\/\\S+\", \"\", tweet)\n",
    "    # remove numbers\n",
    "    tweet = re.sub('\\w*\\d\\w*', ' ', tweet)\n",
    "    # replace '...' with ' '\n",
    "    tweet = re.sub('\\.{2,6}', ' ', tweet)\n",
    "    # remove punctuation\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    # remove repeated letters so spell check will work (ex: 'aaaand' --> 'aand')\n",
    "    tweet = re.sub(r\"([a-z])\\1{2,5}\", r'\\1', tweet)\n",
    "    # replace consecutive spaces with one\n",
    "    tweet = ' '.join(tweet.split())\n",
    "    \n",
    "    # custom replacements. multiwords is a list of tuples such as ('white house', 'white_house')\n",
    "    stop_words = list(stopwords.words('english')) + custom_stop_words\n",
    "    all_words = list(words.words('en')) + custom_words\n",
    "    \n",
    "    for old, new in multi_words:\n",
    "        tweet = re.sub(old, new, tweet)\n",
    "        all_words.append(new)\n",
    "    \n",
    "    return tweet, set(stop_words), set(all_words)\n",
    "\n",
    "def tweet_tokenize(tweet, more_stop=None, more_words=None):\n",
    "    \"\"\"\n",
    "    Get all of the tokens in a set of tweets.\n",
    "    Parameters:\n",
    "        - tweets (Series, required)\n",
    "        - more_stop (List, optional): additional stop words to exclude\n",
    "        - more_words (List, optional): additional words to INCLUDE in dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    tweet, stop_words, all_words = clean_tweet(tweet)\n",
    "    \n",
    "    # lemmatize text\n",
    "    all_words = set(all_words)\n",
    "    twt = TweetTokenizer()\n",
    "        \n",
    "    lemm = WordNetLemmatizer()    \n",
    "    tokens = [lemm.lemmatize(token) for token in twt.tokenize(tweet) if token not in stop_words]\n",
    "    tokens = [token for token in tokens if token in all_words]\n",
    "    combined_tokens = ' '.join(tokens)\n",
    "\n",
    "    return combined_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data['tweet'] = data['original'].map(tweet_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(\"pickle/n2_tokenized.pick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['original', 'tweet']].sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_pickle(\"pickle/tweets_df_5000tw.pick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'tweet tokenize me please mr. biden helloaskldjalksfj I  pence    voting rights am asking for a favor continuous breakdown American Americans'\n",
    "\n",
    "tweet_tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-invalid",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_tokens'] = data['tweet'].str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-companion",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = data['num_tokens'] >= 4\n",
    "data[mask].shape\n",
    "data[mask].sample(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "lemm.lemmatize('organizations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_pickle(\"pickle/n2_tokenized.pick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-administrator",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
