{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "authorized-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-holiday",
   "metadata": {},
   "source": [
    "**Kelsey 1-1**\n",
    "\n",
    "- cleaning\n",
    "    - preprocessing until comfortable with words\n",
    "   \n",
    "- sentiment analysis on all tweets\n",
    "    - don't need to do any splitting at this stage\n",
    "    - TextBlob & VaderSentiment first, spacy if the results aren't as expected\n",
    "    \n",
    "- topic modeling\n",
    "    - decide: use all tweets (all topics) at once\n",
    "        - start here\n",
    "        - then can use these as features in the dataFrame and do splitting here\n",
    "    - or: split to trump/biden - then bot/not bot for each\n",
    "    - point here is there are multiple ways to split it\n",
    "        - no right answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "integral-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 40)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "inner-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must uncomment & run the first time to DOWNLOAD NLTK data\n",
    "# I used package identifier 'popular'\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "macro-shepherd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96000, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle(\"pickle/balanced_nov2_tweets.pick\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "advanced-obligation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>trump</th>\n",
       "      <th>biden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31507</th>\n",
       "      <td>1323405540945498116</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>23:24:03</td>\n",
       "      <td>1026844796395180032</td>\n",
       "      <td>ryan_kelley_</td>\n",
       "      <td>@danjlevy @Realeugenelevy @alxsupertramp_ and I voted early for @JoeBiden and are watching some schitts creek to distract us tonight!</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106211</th>\n",
       "      <td>1323382795352244224</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>21:53:40</td>\n",
       "      <td>261487376</td>\n",
       "      <td>alinecvt</td>\n",
       "      <td>@villa_manzoni @ShironRedshift @JoeBiden Let‚Äôs stay positive, I know there‚Äôs justice in this world. #BidenHarris2020 üôèüèª</td>\n",
       "      <td>['bidenharris2020']</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8431</th>\n",
       "      <td>1323412191157039104</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>23:50:28</td>\n",
       "      <td>1117284155241435137</td>\n",
       "      <td>pineorange10</td>\n",
       "      <td>@Sagesseforever @sebastianjonasw @evan_graham04 @JoeBiden There's evidence lady   https://t.co/SgIYyXKccN</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id       date      time              user_id  \\\n",
       "31507   1323405540945498116 2020-11-02  23:24:03  1026844796395180032   \n",
       "106211  1323382795352244224 2020-11-02  21:53:40            261487376   \n",
       "8431    1323412191157039104 2020-11-02  23:50:28  1117284155241435137   \n",
       "\n",
       "            username  \\\n",
       "31507   ryan_kelley_   \n",
       "106211      alinecvt   \n",
       "8431    pineorange10   \n",
       "\n",
       "                                                                                                                                        tweet  \\\n",
       "31507   @danjlevy @Realeugenelevy @alxsupertramp_ and I voted early for @JoeBiden and are watching some schitts creek to distract us tonight!   \n",
       "106211                @villa_manzoni @ShironRedshift @JoeBiden Let‚Äôs stay positive, I know there‚Äôs justice in this world. #BidenHarris2020 üôèüèª   \n",
       "8431                                @Sagesseforever @sebastianjonasw @evan_graham04 @JoeBiden There's evidence lady   https://t.co/SgIYyXKccN   \n",
       "\n",
       "                   hashtags  trump  biden  \n",
       "31507                    []  False   True  \n",
       "106211  ['bidenharris2020']  False   True  \n",
       "8431                     []  False   True  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-muscle",
   "metadata": {},
   "source": [
    "Now let's create a subset, containing the same amount of Trump tweets as Biden tweets. We will exclude tweets that mention both candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "regional-mainland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>trump</th>\n",
       "      <th>biden</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181142</th>\n",
       "      <td>1323379284434669568</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>21:39:43</td>\n",
       "      <td>2820503362</td>\n",
       "      <td>artistacriseida</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>All these articles showing that Biden is in the lead.... IGNORE THAT AND STILL GO VOTE. All of these maps showing information that may or may not be correct won‚Äôt matter on Election Day. Hillary was also in the lead last election, just do your part.   ‚ÅΩ·∂†·µò·∂ú·µè ·µó ≥·µò·µê·µñ‚Åæ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1323414585995526144</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>23:59:59</td>\n",
       "      <td>1312487180258820096</td>\n",
       "      <td>annapieters17</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>@FoxNews Lady Gaga‚Äôs a nobody. Can‚Äôt figure out her own life and can‚Äôt even see nobody can help Biden. He‚Äôs out of the game from the day he gets in the game.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1323414585232293888</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>23:59:59</td>\n",
       "      <td>2335763630</td>\n",
       "      <td>kylechwatt</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>@The_Grupp ‚ÄúIt is purely a fortuity that this isn‚Äôt one of the great mass casualty events in American history,‚Äù Ron Klain, who was Biden‚Äôs chief of staff at the time, said of H1N1 in 2019.‚Äù   https://t.co/Umi317supK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id       date      time              user_id  \\\n",
       "181142  1323379284434669568 2020-11-02  21:39:43           2820503362   \n",
       "0       1323414585995526144 2020-11-02  23:59:59  1312487180258820096   \n",
       "4       1323414585232293888 2020-11-02  23:59:59           2335763630   \n",
       "\n",
       "               username hashtags  trump  biden  \\\n",
       "181142  artistacriseida       []  False   True   \n",
       "0         annapieters17       []  False   True   \n",
       "4            kylechwatt       []  False   True   \n",
       "\n",
       "                                                                                                                                                                                                                                                                        original  \n",
       "181142  All these articles showing that Biden is in the lead.... IGNORE THAT AND STILL GO VOTE. All of these maps showing information that may or may not be correct won‚Äôt matter on Election Day. Hillary was also in the lead last election, just do your part.   ‚ÅΩ·∂†·µò·∂ú·µè ·µó ≥·µò·µê·µñ‚Åæ  \n",
       "0                                                                                                                  @FoxNews Lady Gaga‚Äôs a nobody. Can‚Äôt figure out her own life and can‚Äôt even see nobody can help Biden. He‚Äôs out of the game from the day he gets in the game.  \n",
       "4                                                        @The_Grupp ‚ÄúIt is purely a fortuity that this isn‚Äôt one of the great mass casualty events in American history,‚Äù Ron Klain, who was Biden‚Äôs chief of staff at the time, said of H1N1 in 2019.‚Äù   https://t.co/Umi317supK  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only necessary columns\n",
    "data['original'] = data.tweet\n",
    "data.drop(columns='tweet', inplace=True)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-highway",
   "metadata": {},
   "source": [
    "## Pre-Processing Pipeline\n",
    "\n",
    "Now it's time to tokenize our tweets. Here are our pre-processing steps:\n",
    "* Remove URLs\n",
    "* Remove Twitter handles\n",
    "* Remove numbers\n",
    "* Convert to lowercase\n",
    "* Remove punctuation\n",
    "* Remove repeated letters so spell check will work ('aaaaand' -> 'aand')\n",
    "* Remove non-English words\n",
    "* Remove stop words\n",
    "\n",
    "Since we're working with so many different words, I've chosen to use **lemmatization** instead of stemming for two reasons:\n",
    "1. Lemmatization accurately reduces words to true meaning\n",
    "2. Inxreased word reduction (handles synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "anticipated-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download() # must run first time (download 'popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interracial-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autocorrect import Speller # TOO SLOW...TRY PYSPELLCHECKER\n",
    "# custom word dictionaries\n",
    "from more_words import more_words as custom_words\n",
    "from stop_words import stop_words as custom_stop_words\n",
    "from multi_words import multi_words\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import words, stopwords\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    # pre-processing pipeline\n",
    "    \n",
    "    # convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # convert 'U.S.' --> 'usa'\n",
    "    tweet = re.sub(r\"u\\.s\\. \", \"usa\", tweet)\n",
    "    # remove urls\n",
    "    tweet = re.sub(r\"https?:\\/\\/\\S+\", \"\", tweet)\n",
    "    # remove numbers\n",
    "    tweet = re.sub('\\w*\\d\\w*', ' ', tweet)\n",
    "    # replace '...' with ' '\n",
    "    tweet = re.sub('\\.{2,6}', ' ', tweet)\n",
    "    # remove punctuation\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    # remove repeated letters so spell check will work (ex: 'aaaand' --> 'aand')\n",
    "    tweet = re.sub(r\"([a-z])\\1{2,5}\", r'\\1', tweet)\n",
    "    # replace consecutive spaces with one\n",
    "    tweet = ' '.join(tweet.split())\n",
    "    \n",
    "    # custom replacements. multiwords is a list of tuples such as ('white house', 'white_house')\n",
    "    stop_words = list(stopwords.words('english')) + custom_stop_words\n",
    "    all_words = list(words.words('en')) + custom_words\n",
    "    \n",
    "    for old, new in multi_words:\n",
    "        tweet = re.sub(old, new, tweet)\n",
    "        all_words.append(new)\n",
    "    \n",
    "    return tweet, set(stop_words), set(all_words)\n",
    "\n",
    "def tweet_tokenize(tweet, more_stop=None, more_words=None):\n",
    "    \"\"\"\n",
    "    Get all of the tokens in a set of tweets.\n",
    "    Parameters:\n",
    "        - tweets (Series, required)\n",
    "        - more_stop (List, optional): additional stop words to exclude\n",
    "        - more_words (List, optional): additional words to INCLUDE in dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    tweet, stop_words, all_words = clean_tweet(tweet)\n",
    "    \n",
    "    # lemmatize text\n",
    "    all_words = set(all_words)\n",
    "    twt = TweetTokenizer()\n",
    "        \n",
    "    lemm = WordNetLemmatizer()    \n",
    "    tokens = [lemm.lemmatize(token) for token in twt.tokenize(tweet) if token in all_words and token not in stop_words]\n",
    "    combined_tokens = ' '.join(tokens)\n",
    "\n",
    "    return combined_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data['tweet'] = data['original'].map(tweet_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(\"pickle/n2_tokenized.pick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['original', 'tweet']].sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_pickle(\"pickle/tweets_df_5000tw.pick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mw in multi_words:\n",
    "    print(mw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'tweet tokenize me please mr. biden helloaskldjalksfj I  pence    voting rights am asking for a favor continuous breakdown American Americans'\n",
    "\n",
    "tweet_tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'left wing hispanic vote white house'\n",
    "for old, new in multi_words:\n",
    "    text = re.sub(old, new, text)\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-columbus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_pickle(\"pickle/n2_tokenized.pick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-bryan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
