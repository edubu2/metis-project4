{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mounted-montana",
   "metadata": {},
   "source": [
    "## Implementing Chris Doenlen's 'Bot Or Not' Python Module\n",
    "\n",
    "Everything from `twitter_funcs.py` was cloned from Chris' [repository](https://github.com/scrapfishies/twitter-bot-detection).\n",
    "\n",
    "I will use this to label each user as 'bot' (boolean 1/0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cellular-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from twitter_funcs import *\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import tweepy\n",
    "\n",
    "from datetime import datetime\n",
    "from secrets import api_secret_key, api_key, bearer_token\n",
    "import re\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "artificial-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"pickle/n2_tokenized.pick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "frozen-rolling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trump</th>\n",
       "      <th>biden</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_id</th>\n",
       "      <th>original</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181142</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>2820503362</td>\n",
       "      <td>All these articles showing that Biden is in th...</td>\n",
       "      <td>showing joe_biden lead ignore still vote showi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>1312487180258820096</td>\n",
       "      <td>@FoxNews Lady Gaga’s a nobody. Can’t figure ou...</td>\n",
       "      <td>lady nobody figure life even see nobody help j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>2335763630</td>\n",
       "      <td>@The_Grupp “It is purely a fortuity that this ...</td>\n",
       "      <td>purely fortuity great mass casualty history jo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        trump  biden hashtags              user_id  \\\n",
       "181142  False   True       []           2820503362   \n",
       "0       False   True       []  1312487180258820096   \n",
       "4       False   True       []           2335763630   \n",
       "\n",
       "                                                 original  \\\n",
       "181142  All these articles showing that Biden is in th...   \n",
       "0       @FoxNews Lady Gaga’s a nobody. Can’t figure ou...   \n",
       "4       @The_Grupp “It is purely a fortuity that this ...   \n",
       "\n",
       "                                                    tweet  \n",
       "181142  showing joe_biden lead ignore still vote showi...  \n",
       "0       lady nobody figure life even see nobody help j...  \n",
       "4       purely fortuity great mass casualty history jo...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-vessel",
   "metadata": {},
   "source": [
    "I'm getting a rate limit error. according to twitter site, I can lookup 300 users per 15 minutes. Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "academic-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(user_ids, n):\n",
    "    \"\"\"Yield successive n-sized chunks from user_ids (iterable).\"\"\"\n",
    "    lst = list(user_ids)\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-village",
   "metadata": {},
   "source": [
    "Here's where we'll implement Chris Doenlen's 'Bot or Not' model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mysterious-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bot_model.pick\", \"rb\") as read_file:\n",
    "    xgb_model = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "verifieds = {}\n",
    "bot_probas = {}\n",
    "\n",
    "all_users = list(data.user_id.unique())\n",
    "exist = pd.read_csv(\"../data/user_stats.csv\")\n",
    "exist_users = list(exist.user_id.unique())\n",
    "\n",
    "user_ids = []\n",
    "for user in exist_users:\n",
    "    if user in all_users:\n",
    "        continue\n",
    "    user_ids.append(user)\n",
    "    \n",
    "print(f\"Preparing to identify bots for {len(user_ids)} users...\")\n",
    "\n",
    "user_id_chunks = list(chunks(user_ids, n=300))\n",
    "\n",
    "# now get stats for new users\n",
    "\n",
    "csv_file = open(\"../data/user_stats.csv\", \"a\")\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow([\"user_id\", \"bot_proba\", \"verified\"])\n",
    "for chunk in user_id_chunks:\n",
    "    print(f\"Preparing chunk. Num users: {len(chunk)}\")\n",
    "    for user_id in chunk:\n",
    "        print(f\"Preparing user '{user_id}'\")\n",
    "        \n",
    "        \n",
    "        auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "        api = tweepy.API(auth)\n",
    "        \n",
    "        try: # Gather features for bot/not bot model\n",
    "            # Get user information from screen name\n",
    "            user = api.get_user(user_id)\n",
    "\n",
    "            # account features to return for predicton\n",
    "            account_age_days = (datetime.now() - user.created_at).days\n",
    "            verified = user.verified # will also use this in our data\n",
    "            geo_enabled = user.geo_enabled\n",
    "            default_profile = user.default_profile\n",
    "            default_profile_image = user.default_profile_image\n",
    "            favourites_count = user.favourites_count\n",
    "            followers_count = user.followers_count\n",
    "            friends_count = user.friends_count\n",
    "            statuses_count = user.statuses_count\n",
    "            average_tweets_per_day = np.round(statuses_count / account_age_days, 3)\n",
    "\n",
    "            # manufactured features\n",
    "            hour_created = int(user.created_at.strftime(\"%H\"))\n",
    "            network = np.round(np.log(1 + friends_count) * np.log(1 + followers_count), 3)\n",
    "            tweet_to_followers = np.round(\n",
    "                np.log(1 + statuses_count) * np.log(1 + followers_count), 3\n",
    "            )\n",
    "            follower_acq_rate = np.round(\n",
    "                np.log(1 + (followers_count / account_age_days)), 3\n",
    "            )\n",
    "            friends_acq_rate = np.round(np.log(1 + (friends_count / account_age_days)), 3)\n",
    "\n",
    "            # organizing list to be returned\n",
    "            account_features = [\n",
    "                verified, hour_created,geo_enabled,default_profile,default_profile_image,favourites_count,\n",
    "                followers_count,friends_count,statuses_count,average_tweets_per_day,network,tweet_to_followers,\n",
    "                follower_acq_rate,friends_acq_rate]\n",
    "\n",
    "            if account_features == np.nan:\n",
    "                proba = np.nan\n",
    "                verified = np.nan\n",
    "                csv_writer.writerow([user_id, proba, verified])\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                user_m = np.matrix(account_features)\n",
    "                proba = np.round(xgb_model.predict_proba(user_m)[:, 1][0] * 100, 2)\n",
    "                verified = account_features[0]\n",
    "                csv_writer.writerow([user_id, proba, verified])\n",
    "\n",
    "        except:\n",
    "            print(f'error encountered, skipping user {user_id}')\n",
    "            proba = np.nan\n",
    "            verified = np.nan\n",
    "        \n",
    "            csv_writer.writerow([user_id, proba, verified])\n",
    "    print(\"Chunk complete. Waiting 15 minutes.\")\n",
    "    time.sleep(15*60+1)\n",
    "\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['bot_proba'] = data['user_id'].map(bot_probas)\n",
    "data['verifieds'] = data['user_id'].map(is_verified)\n",
    "data.to_pickle(\"pickle/tw_proba_verif.pick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "verifieds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['verified'] = data['user_id'].map(verifieds)\n",
    "data['bot_proba'] = data['user_id'].map(bot_probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-relevance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
